import Groq from "groq-sdk";

const client = new Groq({
  apiKey: process.env["GROQ_API_KEY"], // This is the default and can be omitted
});

const evalSystemPrompt = `You are an AI evaluation assistant tasked with analyzing and scoring language models based on their responses to prompts. You will calculate evaluation metrics by comparing the **expected response** to the **real response** generated by the language model.

### Input:
You will receive the following:
1. **Prompt**: The input question or instruction given to the language model.
2. **Expected Response**: The ideal or ground truth response to the prompt.
3. **Real Response**: The actual response generated by the language model.

### Evaluation Metrics:
Evaluate the **Real Response** against the **Expected Response** using the following metrics:
1. **Accuracy** (%): Measure how much the real response aligns with the expected response.
   - 100% for fully correct responses.
   - Scale down for missing or incorrect information.
2. **Hallucination Rate** (%): Measure the extent of fabricated or incorrect information in the real response.
   - 0% if there's no hallucination.
   - Scale up for each factually incorrect or unrelated statement.
3. **Relevance** (1-10): Rate how directly the real response addresses the prompt.
4. **Coherence** (1-10): Evaluate the logical flow, structure, and readability of the real response.
5. **Response Completeness** (%): Measure how much of the expected response is covered in the real response.
6. **Response Length**: Character count or word count comparison between the expected and real responses.

### Output Format:
Provide the evaluation results in the following JSON structure: ONLY GIVE ME THE RESPONSE AS A JSON OBJECT, NO NEED FOR EXPLANATION
json
{
  "prompt": "Explain the theory of relativity in simple terms.",
  "expected_response": "The theory of relativity, developed by Albert Einstein, explains how space and time are interconnected...",
  "real_response": "Einstein's theory explains how time is affected by motion and gravity...",
  "metrics": {
    "accuracy": 95,
    "hallucination_rate": 5,
    "relevance": 9,
    "coherence": 9.5,
    "response_completeness": 85,
    "response_length": {
      "expected": 120,
      "real": 100
    }
  }
}
`;

const aiResponse = async (
  model: string,
  prompt: string,
  systemPrompt: string
) => {
  const chatCompletion = await client.chat.completions.create({
    messages: [
      { role: "user", content: prompt },
      {
        role: "system",
        content: systemPrompt,
      },
    ],
    model: model,
  });
  const response = chatCompletion.choices[0].message.content;
  return response;
};

export async function POST(request: Request) {
  const { prompt, expectedResponse } = await request.json();
  const systemPrompt =
    "You are a cool chatbot, whose response are concise and highly effective, now answer the question: ";
  const gemma2Response = await aiResponse("gemma2-9b-it", prompt, systemPrompt);
  const mixtralResponse = await aiResponse(
    "mixtral-8x7b-32768",
    prompt,
    systemPrompt
  );
  const llamaResponse = await aiResponse(
    "llama-3.3-70b-versatile",
    prompt,
    systemPrompt
  );

  let evalUserPrompt = `prompt: ${prompt} \n
  expected_response: ${expectedResponse} \n
  real_response: ${gemma2Response} \
  `;

  const gemma2EvalPromptResponse = await aiResponse(
    "llama-3.3-70b-versatile",
    evalUserPrompt,
    evalSystemPrompt
  );
  console.log(gemma2EvalPromptResponse);

  evalUserPrompt = `prompt: ${prompt} \n
  expected_response: ${expectedResponse} \n
  real_response: ${mixtralResponse} \
  `;

  const mixtralEvalPromptResponse = await aiResponse(
    "llama-3.3-70b-versatile",
    evalUserPrompt,
    evalSystemPrompt
  );
  console.log(mixtralEvalPromptResponse);

  evalUserPrompt = `prompt: ${prompt} \n
  expected_response: ${expectedResponse} \n
  real_response: ${llamaResponse} \
  `;

  const llamaEvalPromptResponse = await aiResponse(
    "llama-3.3-70b-versatile",
    evalUserPrompt,
    evalSystemPrompt
  );
  console.log(llamaEvalPromptResponse);

  return new Response(
    JSON.stringify({
      gemma2Response,
      mixtralResponse,
      llamaResponse,
      gemma2EvalPromptResponse,
      mixtralEvalPromptResponse,
      llamaEvalPromptResponse,
    }),
    { status: 200 }
  );
}

// ```
// {
//   "prompt": "who is current US president",
//   "expected_response": "Joe Biden",
//   "real_response": "Joe Biden.",
//   "metrics": {
//     "accuracy": 100,
//     "hallucination_rate": 0,
//     "relevance": 10,
//     "coherence": 10,
//     "response_completeness": 100,
//     "response_length": {
//       "expected": 9,
//       "real": 10
//     }
//   }
// }
